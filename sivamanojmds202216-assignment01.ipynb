{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8117512,"sourceType":"datasetVersion","datasetId":4796162},{"sourceId":8117844,"sourceType":"datasetVersion","datasetId":4796374},{"sourceId":8117870,"sourceType":"datasetVersion","datasetId":4796392}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import glob\nimport os\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"execution":{"iopub.status.busy":"2024-05-03T04:58:02.114007Z","iopub.execute_input":"2024-05-03T04:58:02.114372Z","iopub.status.idle":"2024-05-03T04:58:03.177383Z","shell.execute_reply.started":"2024-05-03T04:58:02.114345Z","shell.execute_reply":"2024-05-03T04:58:03.176394Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom gensim.models import KeyedVectors\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Masking, Dropout\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.preprocessing import MultiLabelBinarizer","metadata":{"execution":{"iopub.status.busy":"2024-05-03T05:14:51.668220Z","iopub.execute_input":"2024-05-03T05:14:51.668949Z","iopub.status.idle":"2024-05-03T05:14:51.675496Z","shell.execute_reply.started":"2024-05-03T05:14:51.668919Z","shell.execute_reply":"2024-05-03T05:14:51.674572Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"### 1) Pre - Processing","metadata":{}},{"cell_type":"code","source":"# parameters for pre - processing\nmax_length = 15 # maximum sentence length\nword_dim = 300 # word dimension\n","metadata":{"execution":{"iopub.status.busy":"2024-05-03T04:58:29.863644Z","iopub.execute_input":"2024-05-03T04:58:29.864244Z","iopub.status.idle":"2024-05-03T04:58:29.868905Z","shell.execute_reply.started":"2024-05-03T04:58:29.864217Z","shell.execute_reply":"2024-05-03T04:58:29.867775Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"#### converting each sentence as a list of words with maximum of \"max_length\" words in a sentence. Any sentence greater than \"max_length\" words is splitted. ","metadata":{}},{"cell_type":"code","source":"total_tags = []\nsentences = []\ninput_folder = glob.glob(r'/kaggle/input/text-pos/*')\n\nfor pos_file in input_folder:\n    try:\n        with open(pos_file, 'r') as file:\n            # Read the contents of the file\n            pos_data = file.read()\n            a = pos_data.split()\n            temp_words = []\n            temp_tags = []\n            for i in a:\n                temp_words.append(i.rsplit(\"/\",1)[0].lower())\n                temp_tags.append(i.rsplit(\"/\",1)[1])\n                if i[-1] == \".\" or len(temp_words) == max_length:  # maximum length of sentence is 15  any sentence more than 15 length is shortened to 15.\n                    sentences.append(temp_words)\n                    total_tags.append(temp_tags)\n                    temp_words, temp_tags = [],[]\n    except FileNotFoundError:\n        print(f\"File '{pos_file}' not found.\")","metadata":{"execution":{"iopub.status.busy":"2024-05-03T04:58:29.871736Z","iopub.execute_input":"2024-05-03T04:58:29.872100Z","iopub.status.idle":"2024-05-03T04:58:30.564585Z","shell.execute_reply.started":"2024-05-03T04:58:29.872068Z","shell.execute_reply":"2024-05-03T04:58:30.563779Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"total_words = [i for sublist in sentences for i in sublist]\nflatten_tags = [i for sublist in total_tags for i in sublist]\nunique_words = set(total_words)\nunique_tags = set(flatten_tags)\n\nprint(\"total number of sentences are\", len(sentences))\nprint(\"total number of words are\",len(total_words))\nprint(\"total number of tags are\",len(flatten_tags))\nprint(\"total unique words are\",len(unique_words))\nprint(\"total unique tags are\",len(unique_tags))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-03T04:58:30.565596Z","iopub.execute_input":"2024-05-03T04:58:30.565885Z","iopub.status.idle":"2024-05-03T04:58:30.593365Z","shell.execute_reply.started":"2024-05-03T04:58:30.565862Z","shell.execute_reply":"2024-05-03T04:58:30.592446Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"total number of sentences are 8081\ntotal number of words are 94156\ntotal number of tags are 94156\ntotal unique words are 10955\ntotal unique tags are 46\n","output_type":"stream"}]},{"cell_type":"code","source":"# from transformers import BertModel, BertTokenizer\n\n# # Load pre-trained BERT tokenizer and model\n# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n# model = BertModel.from_pretrained('bert-base-uncased')\n\n# # Convert words into input IDs using the tokenizer\n# input_ids = [tokenizer.convert_tokens_to_ids(words) for words in sentences]\n\n# # Pad sequences to the same length\n# max_length = max(len(ids) for ids in input_ids)\n# padded_input_ids = [ids + [tokenizer.pad_token_id] * (max_length - len(ids)) for ids in input_ids]\n\n# # Convert lists of input IDs to PyTorch tensors\n# input_tensors = torch.tensor(padded_input_ids)\n\n# # Forward pass through the BERT model to obtain contextualized embeddings\n# with torch.no_grad():\n#     outputs = model(input_tensors)\n\n# # Get the contextualized word embeddings (output of the last layer)\n# last_hidden_states = outputs.last_hidden_state\n\n# # Print the shape of the contextualized word embeddings\n# print(\"Shape of contextualized word embeddings:\", last_hidden_states.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T19:03:22.040201Z","iopub.execute_input":"2024-04-14T19:03:22.040498Z","iopub.status.idle":"2024-04-14T19:03:22.048114Z","shell.execute_reply.started":"2024-04-14T19:03:22.040472Z","shell.execute_reply":"2024-04-14T19:03:22.047314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### I'm creating a dictionary where each word in my vocabulary is a key, and the corresponding value for each word is its word vector obtained from GloVe embeddings. This will help me map words to their vector representations for future use.","metadata":{}},{"cell_type":"code","source":"# 1. Download and load GloVe embeddings\ndef load_glove_embeddings(file_path):\n    embeddings = {}\n    with open(file_path, 'r', encoding='utf-8') as file:\n        for line in file:\n            values = line.split()\n            word = values[0]\n            vector = np.array(values[1:], dtype='float32')\n            embeddings[word] = vector\n    return embeddings\n\n# Adjust the file path to point to your downloaded GloVe embeddings file \nglove_file = r'/kaggle/input/pos-tag-text-analytics/glove.6B.300d.txt'\n# word vectors of dimesnion 300\nglove_embeddings = load_glove_embeddings(glove_file)\n\n# 3. Get word vectors for a list of words\nword_vectors = {}\nno_word_vectors = []\nfor word in unique_words:\n    if word in glove_embeddings:\n        # the word present in glove embeddings\n        word_vectors[word] = glove_embeddings[word]\n    else:\n        # the word is not in glove embeddings\n        no_word_vectors.append(word)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T04:58:30.594547Z","iopub.execute_input":"2024-05-03T04:58:30.594888Z","iopub.status.idle":"2024-05-03T04:59:04.467312Z","shell.execute_reply.started":"2024-05-03T04:58:30.594864Z","shell.execute_reply":"2024-05-03T04:59:04.466475Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"####  If a word doesn't have a GloVe vector, I'm assigning it a zero vector.\"\n","metadata":{}},{"cell_type":"code","source":"base_array = np.zeros(word_dim,dtype = np.float64)\nfor word in no_word_vectors:\n    word_vectors[word] = base_array","metadata":{"execution":{"iopub.status.busy":"2024-05-03T04:59:04.468385Z","iopub.execute_input":"2024-05-03T04:59:04.468691Z","iopub.status.idle":"2024-05-03T04:59:04.473763Z","shell.execute_reply.started":"2024-05-03T04:59:04.468656Z","shell.execute_reply":"2024-05-03T04:59:04.472880Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"#### Transforming a sentence, represented as a list of words, into its vector representation","metadata":{}},{"cell_type":"code","source":"sentence_vectors = []\n\n# Word Vectors  --- sentences (words) ---> sentences  (word vectors)\nfor sentence in sentences:\n    temp_sentence = []\n    for word in sentence:\n        temp_sentence.append(word_vectors[word])\n    sentence_vectors.append(temp_sentence)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-03T04:59:04.475194Z","iopub.execute_input":"2024-05-03T04:59:04.475600Z","iopub.status.idle":"2024-05-03T04:59:04.526030Z","shell.execute_reply.started":"2024-05-03T04:59:04.475568Z","shell.execute_reply":"2024-05-03T04:59:04.525137Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"#### Converting a sentence into a list of part-of-speech tags, where each tag is represented by a numerical label.","metadata":{}},{"cell_type":"code","source":"# Label Encoding for tags  --- sentences (words) ---> sentences  (labels or numbers)\nunique_tags.add(\"no_word\")\nle = LabelEncoder()\nle.fit(list(unique_tags))\ntotal_tags_y = [le.transform(i) for i in total_tags] \nprint(\"Sentences represented as pos tagging (after labelling)\\nExample for 3 Sentences\")\ntotal_tags_y[:3]","metadata":{"execution":{"iopub.status.busy":"2024-05-03T04:59:04.526974Z","iopub.execute_input":"2024-05-03T04:59:04.527216Z","iopub.status.idle":"2024-05-03T04:59:05.401206Z","shell.execute_reply.started":"2024-05-03T04:59:04.527196Z","shell.execute_reply":"2024-05-03T04:59:05.400246Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Sentences represented as pos tagging (after labelling)\nExample for 3 Sentences\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"[array([36, 13, 10, 22, 13, 20, 20, 24, 45, 10, 20, 20, 20,  2,  3]),\n array([20,  5,  9, 22,  5,  1,  9,  4,  5, 19, 22, 13, 10, 22, 13]),\n array([10, 20, 39, 10, 19, 13, 19,  6])]"},"metadata":{}}]},{"cell_type":"markdown","source":"#### sentence into a list of part-of-speech tags, where each tag is represented by a one hot vector","metadata":{}},{"cell_type":"code","source":"# Convert word-level labels to one-hot encoded format\nnum_classes = len(unique_tags)  # Number of classes in your target variable\nword_labels_one_hot = [to_categorical(labels, num_classes=num_classes) for labels in total_tags_y]","metadata":{"execution":{"iopub.status.busy":"2024-05-03T04:59:05.404312Z","iopub.execute_input":"2024-05-03T04:59:05.404649Z","iopub.status.idle":"2024-05-03T04:59:05.664025Z","shell.execute_reply.started":"2024-05-03T04:59:05.404624Z","shell.execute_reply":"2024-05-03T04:59:05.662940Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"no_word_vector = to_categorical(le.transform([\"no_word\"]))\nno_word_vector","metadata":{"execution":{"iopub.status.busy":"2024-05-03T04:59:05.665446Z","iopub.execute_input":"2024-05-03T04:59:05.666265Z","iopub.status.idle":"2024-05-03T04:59:05.673838Z","shell.execute_reply.started":"2024-05-03T04:59:05.666231Z","shell.execute_reply":"2024-05-03T04:59:05.673004Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"},"metadata":{}}]},{"cell_type":"markdown","source":"#### pad_sequence --> any given sequence of length less than \"max_length\" is padded by zero padding at front (word vector dimension).\n\n#### pad_sequence_labels --> labels also padded\n","metadata":{}},{"cell_type":"code","source":"def pad_sequence(sentences,max_length,word_dim):\n    sentence_vectors_padded = []\n    word_labels_one_hot_padded = []\n    \n    for sentence in sentences:\n        to_be_added = max_length - len(sentence)\n        temp_array = np.vstack((np.zeros((to_be_added,word_dim)),np.array(sentence)))\n        sentence_vectors_padded.append(temp_array)\n\n    sentence_vectors_padded = np.array(sentence_vectors_padded)\n    return sentence_vectors_padded\n\ndef pad_sequence_labels(word_labels_one_hot,max_length,label_dim):\n    word_labels_one_hot_padded = []\n    \n    for sentence in word_labels_one_hot:\n        to_be_added = max_length - len(sentence)\n        repeated_a = np.repeat(no_word_vector, to_be_added, axis=0).reshape(-1, no_word_vector.shape[1])\n        temp_array = np.vstack((repeated_a,np.array(sentence)))\n        word_labels_one_hot_padded.append(temp_array)\n\n    word_labels_one_hot_padded = np.array(word_labels_one_hot_padded)\n    return word_labels_one_hot_padded\n","metadata":{"execution":{"iopub.status.busy":"2024-05-03T04:59:05.674796Z","iopub.execute_input":"2024-05-03T04:59:05.675051Z","iopub.status.idle":"2024-05-03T04:59:05.683837Z","shell.execute_reply.started":"2024-05-03T04:59:05.675030Z","shell.execute_reply":"2024-05-03T04:59:05.682994Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"sentence_vectors_padded = pad_sequence(sentence_vectors, max_length,word_dim)\nword_labels_one_hot_padded = pad_sequence_labels(word_labels_one_hot,max_length,47)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T04:59:05.684780Z","iopub.execute_input":"2024-05-03T04:59:05.685033Z","iopub.status.idle":"2024-05-03T04:59:06.228755Z","shell.execute_reply.started":"2024-05-03T04:59:05.685012Z","shell.execute_reply":"2024-05-03T04:59:06.227645Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"sentence_vectors_padded.shape, word_labels_one_hot_padded.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-03T04:59:06.230269Z","iopub.execute_input":"2024-05-03T04:59:06.230551Z","iopub.status.idle":"2024-05-03T04:59:06.240262Z","shell.execute_reply.started":"2024-05-03T04:59:06.230528Z","shell.execute_reply":"2024-05-03T04:59:06.239266Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"((8081, 15, 300), (8081, 15, 47))"},"metadata":{}}]},{"cell_type":"markdown","source":"#### in the above output (number of sentnces, maximum length of sentence, word dimension)\n#### second output is (number of sentences, maximum length of sentence, tag dimension)","metadata":{}},{"cell_type":"markdown","source":"### 2) Train Test Validation Split","metadata":{}},{"cell_type":"code","source":"X = np.array(sentence_vectors_padded)\ny = np.array(word_labels_one_hot_padded)\n\n# Split the dataset into training, validation, and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42)  # 0.15 x 0.8 = 0.12","metadata":{"execution":{"iopub.status.busy":"2024-05-03T06:09:49.246507Z","iopub.execute_input":"2024-05-03T06:09:49.247184Z","iopub.status.idle":"2024-05-03T06:09:49.557582Z","shell.execute_reply.started":"2024-05-03T06:09:49.247153Z","shell.execute_reply":"2024-05-03T06:09:49.556490Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"markdown","source":"### 3) Modelling (LSTM)","metadata":{}},{"cell_type":"code","source":"hidden_units = 100\nnum_features = word_dim\n\nmodel = Sequential([\n    Masking(mask_value=0.0, input_shape=(max_length, num_features)),\n    LSTM(units=hidden_units, return_sequences=True),\n    Dropout(0.2),  # Add dropout for regularization\n    Dense(units=num_classes, activation='softmax')\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Define early stopping callback\nearly_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n\n# Train the model\nhistory = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val), callbacks=[early_stopping])\n","metadata":{"execution":{"iopub.status.busy":"2024-05-03T06:09:50.661223Z","iopub.execute_input":"2024-05-03T06:09:50.662419Z","iopub.status.idle":"2024-05-03T06:11:31.943376Z","shell.execute_reply.started":"2024-05-03T06:09:50.662371Z","shell.execute_reply":"2024-05-03T06:11:31.942613Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/masking.py:47: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50\n\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.2927 - loss: 2.5298 - val_accuracy: 0.5941 - val_loss: 0.9336\nEpoch 2/50\n\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - accuracy: 0.6001 - loss: 0.8451 - val_accuracy: 0.6460 - val_loss: 0.6228\nEpoch 3/50\n\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.6495 - loss: 0.5924 - val_accuracy: 0.6668 - val_loss: 0.4972\nEpoch 4/50\n\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - accuracy: 0.6666 - loss: 0.4807 - val_accuracy: 0.6810 - val_loss: 0.4261\nEpoch 5/50\n\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.6861 - loss: 0.4126 - val_accuracy: 0.6892 - val_loss: 0.3811\nEpoch 6/50\n\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.6931 - loss: 0.3603 - val_accuracy: 0.6988 - val_loss: 0.3456\nEpoch 7/50\n\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - accuracy: 0.7044 - loss: 0.3162 - val_accuracy: 0.7024 - val_loss: 0.3190\nEpoch 8/50\n\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - accuracy: 0.7144 - loss: 0.2812 - val_accuracy: 0.7057 - val_loss: 0.3038\nEpoch 9/50\n\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.7196 - loss: 0.2484 - val_accuracy: 0.7107 - val_loss: 0.2842\nEpoch 10/50\n\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.7249 - loss: 0.2261 - val_accuracy: 0.7117 - val_loss: 0.2723\nEpoch 11/50\n\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - accuracy: 0.7229 - loss: 0.2063 - val_accuracy: 0.7144 - val_loss: 0.2628\nEpoch 12/50\n\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.7258 - loss: 0.1860 - val_accuracy: 0.7155 - val_loss: 0.2536\nEpoch 13/50\n\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.7403 - loss: 0.1721 - val_accuracy: 0.7156 - val_loss: 0.2480\nEpoch 14/50\n\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.7390 - loss: 0.1600 - val_accuracy: 0.7167 - val_loss: 0.2424\nEpoch 15/50\n\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 26ms/step - accuracy: 0.7321 - loss: 0.1455 - val_accuracy: 0.7186 - val_loss: 0.2410\nEpoch 16/50\n\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.7395 - loss: 0.1339 - val_accuracy: 0.7180 - val_loss: 0.2414\nEpoch 17/50\n\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.7455 - loss: 0.1240 - val_accuracy: 0.7191 - val_loss: 0.2368\nEpoch 18/50\n\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.7537 - loss: 0.1174 - val_accuracy: 0.7195 - val_loss: 0.2362\nEpoch 19/50\n\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.7428 - loss: 0.1064 - val_accuracy: 0.7199 - val_loss: 0.2374\nEpoch 20/50\n\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.7531 - loss: 0.0968 - val_accuracy: 0.7213 - val_loss: 0.2353\nEpoch 21/50\n\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - accuracy: 0.7533 - loss: 0.0902 - val_accuracy: 0.7199 - val_loss: 0.2401\nEpoch 22/50\n\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 26ms/step - accuracy: 0.7487 - loss: 0.0883 - val_accuracy: 0.7213 - val_loss: 0.2401\nEpoch 23/50\n\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.7624 - loss: 0.0820 - val_accuracy: 0.7210 - val_loss: 0.2419\n","output_type":"stream"}]},{"cell_type":"code","source":"# Evaluate the model on the test set\ntest_loss, test_accuracy = model.evaluate(X_test, y_test)\nprint(\"Test Loss:\", test_loss)\nprint(\"Test Accuracy:\", test_accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T06:11:31.945331Z","iopub.execute_input":"2024-05-03T06:11:31.945617Z","iopub.status.idle":"2024-05-03T06:11:32.402177Z","shell.execute_reply.started":"2024-05-03T06:11:31.945593Z","shell.execute_reply":"2024-05-03T06:11:32.401292Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7152 - loss: 0.2366\nTest Loss: 0.24418075382709503\nTest Accuracy: 0.712141752243042\n","output_type":"stream"}]},{"cell_type":"code","source":"y_test_pred = model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T06:11:32.403509Z","iopub.execute_input":"2024-05-03T06:11:32.404195Z","iopub.status.idle":"2024-05-03T06:11:33.203076Z","shell.execute_reply.started":"2024-05-03T06:11:32.404167Z","shell.execute_reply":"2024-05-03T06:11:33.202300Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"# to check the accuracy (word level) of model manually\ndef cal_accuracy(y_test, y_test_pred):\n    # Get the index of the class with the highest probability for each word\n    actual_tags_indices = np.argmax(y_test, axis=-1)  # Shape: (1, 15)\n    # Get the index of the class with the highest probability for each word\n    predicted_tags_indices = np.argmax(y_test_pred, axis=-1)  # Shape: (1, 15)\n\n    actual_tags_indices_list = list(actual_tags_indices.flatten())\n    predicted_tags_indices_list = list(predicted_tags_indices.flatten())\n\n    i = 0\n    count = 0\n    while i < len(actual_tags_indices_list):\n        if actual_tags_indices_list[i] == predicted_tags_indices_list[i]:\n            count+=1\n        i+=1\n    accuracy = count/i *100\n    return accuracy","metadata":{"execution":{"iopub.status.busy":"2024-05-03T06:11:33.205344Z","iopub.execute_input":"2024-05-03T06:11:33.205632Z","iopub.status.idle":"2024-05-03T06:11:33.211990Z","shell.execute_reply.started":"2024-05-03T06:11:33.205607Z","shell.execute_reply":"2024-05-03T06:11:33.211074Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"test_accuracy = cal_accuracy(y_test, y_test_pred)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T06:11:33.213061Z","iopub.execute_input":"2024-05-03T06:11:33.213340Z","iopub.status.idle":"2024-05-03T06:11:33.240371Z","shell.execute_reply.started":"2024-05-03T06:11:33.213317Z","shell.execute_reply":"2024-05-03T06:11:33.239687Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"markdown","source":"### 4) Test Results","metadata":{}},{"cell_type":"code","source":"print(\"test accuracy of model is : \", test_accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T06:11:33.241229Z","iopub.execute_input":"2024-05-03T06:11:33.241454Z","iopub.status.idle":"2024-05-03T06:11:33.246111Z","shell.execute_reply.started":"2024-05-03T06:11:33.241434Z","shell.execute_reply":"2024-05-03T06:11:33.245279Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stdout","text":"test accuracy of model is :  71.21418264275408\n","output_type":"stream"}]},{"cell_type":"code","source":"# Reshaping the arrays so that we will have word level labels\ny_test_reshaped = y_test.reshape(-1, 47)\ny_test_pred_reshaped = y_test_pred.reshape(-1,47)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T06:11:33.247092Z","iopub.execute_input":"2024-05-03T06:11:33.247362Z","iopub.status.idle":"2024-05-03T06:11:33.255589Z","shell.execute_reply.started":"2024-05-03T06:11:33.247341Z","shell.execute_reply":"2024-05-03T06:11:33.254776Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"## Converting y_pred to one hot vector\nnum_classes = y_test_reshaped.shape[1]\ny_test_pred_one_hot = np.eye(num_classes)[np.argmax(y_test_pred_reshaped, axis = 1)]","metadata":{"execution":{"iopub.status.busy":"2024-05-03T06:11:33.256645Z","iopub.execute_input":"2024-05-03T06:11:33.257021Z","iopub.status.idle":"2024-05-03T06:11:33.268175Z","shell.execute_reply.started":"2024-05-03T06:11:33.256998Z","shell.execute_reply":"2024-05-03T06:11:33.267293Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"def cal_metrics(y_true, y_pred):\n    # Compute precision, recall, and F1-score for each class\n    precision, recall, f1_score, support = precision_recall_fscore_support(y_true, y_pred, average=None, zero_division=1)\n\n    return precision, recall, f1_score, support\n\n# Example usage:\nprecision, recall, f1_score, support = cal_metrics(y_test_reshaped, y_test_pred_one_hot)\n\n# Create a DataFrame with precision and recall for each class\nmetrics_df = pd.DataFrame({\n    'Class': le.inverse_transform(range(len(precision))),\n    'Precision': precision,\n    'Recall': recall,\n    \"F1 Score\":f1_score,\n    \"Support\":support\n})\n\nprint(\"Classification Report for all Tags\\n\")\nprint(metrics_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-03T06:11:33.269175Z","iopub.execute_input":"2024-05-03T06:11:33.269424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}