{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8117512,"sourceType":"datasetVersion","datasetId":4796162},{"sourceId":8117844,"sourceType":"datasetVersion","datasetId":4796374},{"sourceId":8117870,"sourceType":"datasetVersion","datasetId":4796392}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import glob\nimport os\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"execution":{"iopub.status.busy":"2024-04-14T19:03:21.413077Z","iopub.execute_input":"2024-04-14T19:03:21.413860Z","iopub.status.idle":"2024-04-14T19:03:21.418044Z","shell.execute_reply.started":"2024-04-14T19:03:21.413827Z","shell.execute_reply":"2024-04-14T19:03:21.417056Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom gensim.models import KeyedVectors\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Masking, Dropout\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T19:03:21.422573Z","iopub.execute_input":"2024-04-14T19:03:21.423443Z","iopub.status.idle":"2024-04-14T19:03:21.430103Z","shell.execute_reply.started":"2024-04-14T19:03:21.423410Z","shell.execute_reply":"2024-04-14T19:03:21.429123Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":"### 1) Pre - Processing","metadata":{}},{"cell_type":"code","source":"# parameters for pre - processing\nmax_length = 15 # maximum sentence length\nword_dim = 300 # word dimension\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T19:03:21.431549Z","iopub.execute_input":"2024-04-14T19:03:21.431872Z","iopub.status.idle":"2024-04-14T19:03:21.441264Z","shell.execute_reply.started":"2024-04-14T19:03:21.431847Z","shell.execute_reply":"2024-04-14T19:03:21.440369Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"markdown","source":"#### converting each sentence as a list of words with maximum of \"max_length\" words in a sentence. Any sentence greater than \"max_length\" words is splitted. ","metadata":{}},{"cell_type":"code","source":"total_tags = []\nsentences = []\ninput_folder = glob.glob(r'/kaggle/input/text-pos/*')\n\nfor pos_file in input_folder:\n    try:\n        with open(pos_file, 'r') as file:\n            # Read the contents of the file\n            pos_data = file.read()\n            a = pos_data.split()\n            temp_words = []\n            temp_tags = []\n            for i in a:\n                temp_words.append(i.rsplit(\"/\",1)[0].lower())\n                temp_tags.append(i.rsplit(\"/\",1)[1])\n                if i[-1] == \".\" or len(temp_words) == max_length:  # maximum length of sentence is 15  any sentence more than 15 length is shortened to 15.\n                    sentences.append(temp_words)\n                    total_tags.append(temp_tags)\n                    temp_words, temp_tags = [],[]\n    except FileNotFoundError:\n        print(f\"File '{pos_file}' not found.\")","metadata":{"execution":{"iopub.status.busy":"2024-04-14T19:03:21.442833Z","iopub.execute_input":"2024-04-14T19:03:21.443164Z","iopub.status.idle":"2024-04-14T19:03:21.995214Z","shell.execute_reply.started":"2024-04-14T19:03:21.443140Z","shell.execute_reply":"2024-04-14T19:03:21.994391Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"total_words = [i for sublist in sentences for i in sublist]\nflatten_tags = [i for sublist in total_tags for i in sublist]\nunique_words = set(total_words)\nunique_tags = set(flatten_tags)\n\nprint(\"total number of sentences are\", len(sentences))\nprint(\"total number of words are\",len(total_words))\nprint(\"total number of tags are\",len(flatten_tags))\nprint(\"total unique words are\",len(unique_words))\nprint(\"total unique tags are\",len(unique_tags))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T19:03:21.996807Z","iopub.execute_input":"2024-04-14T19:03:21.997115Z","iopub.status.idle":"2024-04-14T19:03:22.039088Z","shell.execute_reply.started":"2024-04-14T19:03:21.997089Z","shell.execute_reply":"2024-04-14T19:03:22.038119Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"total number of sentences are 8081\ntotal number of words are 94156\ntotal number of tags are 94156\ntotal unique words are 10955\ntotal unique tags are 46\n","output_type":"stream"}]},{"cell_type":"code","source":"# from transformers import BertModel, BertTokenizer\n\n# # Load pre-trained BERT tokenizer and model\n# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n# model = BertModel.from_pretrained('bert-base-uncased')\n\n# # Convert words into input IDs using the tokenizer\n# input_ids = [tokenizer.convert_tokens_to_ids(words) for words in sentences]\n\n# # Pad sequences to the same length\n# max_length = max(len(ids) for ids in input_ids)\n# padded_input_ids = [ids + [tokenizer.pad_token_id] * (max_length - len(ids)) for ids in input_ids]\n\n# # Convert lists of input IDs to PyTorch tensors\n# input_tensors = torch.tensor(padded_input_ids)\n\n# # Forward pass through the BERT model to obtain contextualized embeddings\n# with torch.no_grad():\n#     outputs = model(input_tensors)\n\n# # Get the contextualized word embeddings (output of the last layer)\n# last_hidden_states = outputs.last_hidden_state\n\n# # Print the shape of the contextualized word embeddings\n# print(\"Shape of contextualized word embeddings:\", last_hidden_states.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T19:03:22.040201Z","iopub.execute_input":"2024-04-14T19:03:22.040498Z","iopub.status.idle":"2024-04-14T19:03:22.048114Z","shell.execute_reply.started":"2024-04-14T19:03:22.040472Z","shell.execute_reply":"2024-04-14T19:03:22.047314Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"markdown","source":"#### I'm creating a dictionary where each word in my vocabulary is a key, and the corresponding value for each word is its word vector obtained from GloVe embeddings. This will help me map words to their vector representations for future use.","metadata":{}},{"cell_type":"code","source":"# 1. Download and load GloVe embeddings\ndef load_glove_embeddings(file_path):\n    embeddings = {}\n    with open(file_path, 'r', encoding='utf-8') as file:\n        for line in file:\n            values = line.split()\n            word = values[0]\n            vector = np.array(values[1:], dtype='float32')\n            embeddings[word] = vector\n    return embeddings\n\n# Adjust the file path to point to your downloaded GloVe embeddings file \nglove_file = r'/kaggle/input/pos-tag-text-analytics/glove.6B.300d.txt'\n# word vectors of dimesnion 300\nglove_embeddings = load_glove_embeddings(glove_file)\n\n# 3. Get word vectors for a list of words\nword_vectors = {}\nno_word_vectors = []\nfor word in unique_words:\n    if word in glove_embeddings:\n        # the word present in glove embeddings\n        word_vectors[word] = glove_embeddings[word]\n    else:\n        # the word is not in glove embeddings\n        no_word_vectors.append(word)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T19:03:22.050064Z","iopub.execute_input":"2024-04-14T19:03:22.050395Z","iopub.status.idle":"2024-04-14T19:03:53.895641Z","shell.execute_reply.started":"2024-04-14T19:03:22.050370Z","shell.execute_reply":"2024-04-14T19:03:53.894805Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"markdown","source":"####  If a word doesn't have a GloVe vector, I'm assigning it a zero vector.\"\n","metadata":{}},{"cell_type":"code","source":"base_array = np.zeros(word_dim,dtype = np.float64)\nfor word in no_word_vectors:\n    word_vectors[word] = base_array","metadata":{"execution":{"iopub.status.busy":"2024-04-14T19:03:53.896707Z","iopub.execute_input":"2024-04-14T19:03:53.897002Z","iopub.status.idle":"2024-04-14T19:03:53.902227Z","shell.execute_reply.started":"2024-04-14T19:03:53.896980Z","shell.execute_reply":"2024-04-14T19:03:53.901250Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"markdown","source":"#### Transforming a sentence, represented as a list of words, into its vector representation","metadata":{}},{"cell_type":"code","source":"sentence_vectors = []\n\n# Word Vectors  --- sentences (words) ---> sentences  (word vectors)\nfor sentence in sentences:\n    temp_sentence = []\n    for word in sentence:\n        temp_sentence.append(word_vectors[word])\n    sentence_vectors.append(temp_sentence)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T19:03:53.903479Z","iopub.execute_input":"2024-04-14T19:03:53.903803Z","iopub.status.idle":"2024-04-14T19:03:53.975870Z","shell.execute_reply.started":"2024-04-14T19:03:53.903777Z","shell.execute_reply":"2024-04-14T19:03:53.975099Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"markdown","source":"#### Converting a sentence into a list of part-of-speech tags, where each tag is represented by a numerical label.","metadata":{}},{"cell_type":"code","source":"# Label Encoding for tags  --- sentences (words) ---> sentences  (labels or numbers)\nunique_tags.add(\"no_word\")\nle = LabelEncoder()\nle.fit(list(unique_tags))\ntotal_tags_y = [le.transform(i) for i in total_tags] \nprint(\"Sentences represented as pos tagging (after labelling)\\nExample for 3 Sentences\")\ntotal_tags_y[:3]","metadata":{"execution":{"iopub.status.busy":"2024-04-14T19:03:53.976990Z","iopub.execute_input":"2024-04-14T19:03:53.977255Z","iopub.status.idle":"2024-04-14T19:03:54.930380Z","shell.execute_reply.started":"2024-04-14T19:03:53.977233Z","shell.execute_reply":"2024-04-14T19:03:54.929344Z"},"trusted":true},"execution_count":81,"outputs":[{"name":"stdout","text":"Sentences represented as pos tagging (after labelling)\nExample for 3 Sentences\n","output_type":"stream"},{"execution_count":81,"output_type":"execute_result","data":{"text/plain":"[array([36, 13, 10, 22, 13, 20, 20, 24, 45, 10, 20, 20, 20,  2,  3]),\n array([20,  5,  9, 22,  5,  1,  9,  4,  5, 19, 22, 13, 10, 22, 13]),\n array([10, 20, 39, 10, 19, 13, 19,  6])]"},"metadata":{}}]},{"cell_type":"markdown","source":"#### sentence into a list of part-of-speech tags, where each tag is represented by a one hot vector","metadata":{}},{"cell_type":"code","source":"# Convert word-level labels to one-hot encoded format\nnum_classes = len(unique_tags)  # Number of classes in your target variable\nword_labels_one_hot = [to_categorical(labels, num_classes=num_classes) for labels in total_tags_y]","metadata":{"execution":{"iopub.status.busy":"2024-04-14T19:03:54.931571Z","iopub.execute_input":"2024-04-14T19:03:54.931914Z","iopub.status.idle":"2024-04-14T19:03:55.187137Z","shell.execute_reply.started":"2024-04-14T19:03:54.931869Z","shell.execute_reply":"2024-04-14T19:03:55.186145Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"no_word_vector = to_categorical(le.transform([\"no_word\"]))\nno_word_vector","metadata":{"execution":{"iopub.status.busy":"2024-04-14T19:03:55.196064Z","iopub.execute_input":"2024-04-14T19:03:55.196366Z","iopub.status.idle":"2024-04-14T19:03:55.203758Z","shell.execute_reply.started":"2024-04-14T19:03:55.196340Z","shell.execute_reply":"2024-04-14T19:03:55.202937Z"},"trusted":true},"execution_count":83,"outputs":[{"execution_count":83,"output_type":"execute_result","data":{"text/plain":"array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"},"metadata":{}}]},{"cell_type":"markdown","source":"#### pad_sequence --> any given sequence of length less than \"max_length\" is padded by zero padding at front (word vector dimension).\n\n#### pad_sequence_labels --> labels also padded\n","metadata":{}},{"cell_type":"code","source":"def pad_sequence(sentences,max_length,word_dim):\n    sentence_vectors_padded = []\n    word_labels_one_hot_padded = []\n    \n    for sentence in sentences:\n        to_be_added = max_length - len(sentence)\n        temp_array = np.vstack((np.zeros((to_be_added,word_dim)),np.array(sentence)))\n        sentence_vectors_padded.append(temp_array)\n\n    sentence_vectors_padded = np.array(sentence_vectors_padded)\n    return sentence_vectors_padded\n\ndef pad_sequence_labels(word_labels_one_hot,max_length,label_dim):\n    word_labels_one_hot_padded = []\n    \n    for sentence in word_labels_one_hot:\n        to_be_added = max_length - len(sentence)\n        repeated_a = np.repeat(no_word_vector, to_be_added, axis=0).reshape(-1, no_word_vector.shape[1])\n        temp_array = np.vstack((repeated_a,np.array(sentence)))\n        word_labels_one_hot_padded.append(temp_array)\n\n    word_labels_one_hot_padded = np.array(word_labels_one_hot_padded)\n    return word_labels_one_hot_padded\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T19:03:55.207828Z","iopub.execute_input":"2024-04-14T19:03:55.208173Z","iopub.status.idle":"2024-04-14T19:03:55.216288Z","shell.execute_reply.started":"2024-04-14T19:03:55.208142Z","shell.execute_reply":"2024-04-14T19:03:55.215372Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"sentence_vectors_padded = pad_sequence(sentence_vectors, max_length,word_dim)\nword_labels_one_hot_padded = pad_sequence_labels(word_labels_one_hot,max_length,47)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T19:03:55.217533Z","iopub.execute_input":"2024-04-14T19:03:55.217874Z","iopub.status.idle":"2024-04-14T19:03:55.663474Z","shell.execute_reply.started":"2024-04-14T19:03:55.217844Z","shell.execute_reply":"2024-04-14T19:03:55.661294Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"sentence_vectors_padded.shape, word_labels_one_hot_padded.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-14T19:03:55.664586Z","iopub.execute_input":"2024-04-14T19:03:55.664921Z","iopub.status.idle":"2024-04-14T19:03:55.671200Z","shell.execute_reply.started":"2024-04-14T19:03:55.664866Z","shell.execute_reply":"2024-04-14T19:03:55.670229Z"},"trusted":true},"execution_count":86,"outputs":[{"execution_count":86,"output_type":"execute_result","data":{"text/plain":"((8081, 15, 300), (8081, 15, 47))"},"metadata":{}}]},{"cell_type":"markdown","source":"#### in the above output (number of sentnces, maximum length of sentence, word dimension)\n#### second output is (number of sentences, maximum length of sentence, tag dimension)","metadata":{}},{"cell_type":"markdown","source":"### 2) Train Test Validation Split","metadata":{}},{"cell_type":"code","source":"X = np.array(sentence_vectors_padded)\ny = np.array(word_labels_one_hot_padded)\n\n# Split the dataset into training, validation, and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2","metadata":{"execution":{"iopub.status.busy":"2024-04-14T19:03:55.672323Z","iopub.execute_input":"2024-04-14T19:03:55.672604Z","iopub.status.idle":"2024-04-14T19:03:55.974957Z","shell.execute_reply.started":"2024-04-14T19:03:55.672581Z","shell.execute_reply":"2024-04-14T19:03:55.973968Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"markdown","source":"### 3) Modelling (LSTM)","metadata":{}},{"cell_type":"code","source":"hidden_units = 100\nnum_features = word_dim\n\nmodel = Sequential([\n    Masking(mask_value=0.0, input_shape=(max_length, num_features)),\n    LSTM(units=hidden_units, return_sequences=True),\n    Dropout(0.2),  # Add dropout for regularization\n    Dense(units=num_classes, activation='softmax')\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Define early stopping callback\nearly_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n\n# Train the model\nhistory = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val), callbacks=[early_stopping])\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T19:03:55.976148Z","iopub.execute_input":"2024-04-14T19:03:55.976437Z","iopub.status.idle":"2024-04-14T19:05:43.137434Z","shell.execute_reply.started":"2024-04-14T19:03:55.976412Z","shell.execute_reply":"2024-04-14T19:05:43.136636Z"},"trusted":true},"execution_count":88,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/masking.py:47: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 30ms/step - accuracy: 0.2770 - loss: 2.6102 - val_accuracy: 0.5821 - val_loss: 0.9906\nEpoch 2/50\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - accuracy: 0.5890 - loss: 0.9112 - val_accuracy: 0.6366 - val_loss: 0.6674\nEpoch 3/50\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - accuracy: 0.6415 - loss: 0.6394 - val_accuracy: 0.6573 - val_loss: 0.5387\nEpoch 4/50\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - accuracy: 0.6566 - loss: 0.5237 - val_accuracy: 0.6698 - val_loss: 0.4690\nEpoch 5/50\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - accuracy: 0.6777 - loss: 0.4454 - val_accuracy: 0.6780 - val_loss: 0.4212\nEpoch 6/50\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - accuracy: 0.6824 - loss: 0.3904 - val_accuracy: 0.6842 - val_loss: 0.3889\nEpoch 7/50\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - accuracy: 0.6917 - loss: 0.3464 - val_accuracy: 0.6914 - val_loss: 0.3593\nEpoch 8/50\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - accuracy: 0.6985 - loss: 0.3122 - val_accuracy: 0.6936 - val_loss: 0.3427\nEpoch 9/50\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - accuracy: 0.7120 - loss: 0.2826 - val_accuracy: 0.6993 - val_loss: 0.3220\nEpoch 10/50\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - accuracy: 0.7146 - loss: 0.2548 - val_accuracy: 0.7009 - val_loss: 0.3042\nEpoch 11/50\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - accuracy: 0.7187 - loss: 0.2360 - val_accuracy: 0.7038 - val_loss: 0.2936\nEpoch 12/50\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - accuracy: 0.7240 - loss: 0.2116 - val_accuracy: 0.7057 - val_loss: 0.2830\nEpoch 13/50\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 30ms/step - accuracy: 0.7344 - loss: 0.1872 - val_accuracy: 0.7076 - val_loss: 0.2766\nEpoch 14/50\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - accuracy: 0.7369 - loss: 0.1780 - val_accuracy: 0.7081 - val_loss: 0.2682\nEpoch 15/50\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - accuracy: 0.7296 - loss: 0.1673 - val_accuracy: 0.7093 - val_loss: 0.2642\nEpoch 16/50\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - accuracy: 0.7461 - loss: 0.1500 - val_accuracy: 0.7103 - val_loss: 0.2599\nEpoch 17/50\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - accuracy: 0.7449 - loss: 0.1434 - val_accuracy: 0.7104 - val_loss: 0.2579\nEpoch 18/50\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - accuracy: 0.7422 - loss: 0.1296 - val_accuracy: 0.7109 - val_loss: 0.2581\nEpoch 19/50\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - accuracy: 0.7444 - loss: 0.1193 - val_accuracy: 0.7108 - val_loss: 0.2575\nEpoch 20/50\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 30ms/step - accuracy: 0.7492 - loss: 0.1132 - val_accuracy: 0.7125 - val_loss: 0.2555\nEpoch 21/50\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - accuracy: 0.7469 - loss: 0.1072 - val_accuracy: 0.7128 - val_loss: 0.2536\nEpoch 22/50\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - accuracy: 0.7505 - loss: 0.0958 - val_accuracy: 0.7133 - val_loss: 0.2577\nEpoch 23/50\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - accuracy: 0.7598 - loss: 0.0884 - val_accuracy: 0.7126 - val_loss: 0.2576\nEpoch 24/50\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - accuracy: 0.7493 - loss: 0.0847 - val_accuracy: 0.7134 - val_loss: 0.2603\n","output_type":"stream"}]},{"cell_type":"code","source":"# Evaluate the model on the test set\ntest_loss, test_accuracy = model.evaluate(X_test, y_test)\nprint(\"Test Loss:\", test_loss)\nprint(\"Test Accuracy:\", test_accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T19:05:43.139612Z","iopub.execute_input":"2024-04-14T19:05:43.140288Z","iopub.status.idle":"2024-04-14T19:05:43.635455Z","shell.execute_reply.started":"2024-04-14T19:05:43.140252Z","shell.execute_reply":"2024-04-14T19:05:43.634541Z"},"trusted":true},"execution_count":89,"outputs":[{"name":"stdout","text":"\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7111 - loss: 0.2521\nTest Loss: 0.2591118812561035\nTest Accuracy: 0.7094621658325195\n","output_type":"stream"}]},{"cell_type":"code","source":"y_test_pred = model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T19:05:43.636695Z","iopub.execute_input":"2024-04-14T19:05:43.637010Z","iopub.status.idle":"2024-04-14T19:05:44.515203Z","shell.execute_reply.started":"2024-04-14T19:05:43.636984Z","shell.execute_reply":"2024-04-14T19:05:44.514146Z"},"trusted":true},"execution_count":90,"outputs":[{"name":"stdout","text":"\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"# to check the accuracy (word level) of model manually\ndef cal_accuracy(y_test, y_test_pred):\n    # Get the index of the class with the highest probability for each word\n    actual_tags_indices = np.argmax(y_test, axis=-1)  # Shape: (1, 15)\n    # Get the index of the class with the highest probability for each word\n    predicted_tags_indices = np.argmax(y_test_pred, axis=-1)  # Shape: (1, 15)\n\n    actual_tags_indices_list = list(actual_tags_indices.flatten())\n    predicted_tags_indices_list = list(predicted_tags_indices.flatten())\n\n    i = 0\n    count = 0\n    while i < len(actual_tags_indices_list):\n        if actual_tags_indices_list[i] == predicted_tags_indices_list[i]:\n            count+=1\n        i+=1\n    accuracy = count/i *100\n    return accuracy","metadata":{"execution":{"iopub.status.busy":"2024-04-14T19:05:44.517250Z","iopub.execute_input":"2024-04-14T19:05:44.517770Z","iopub.status.idle":"2024-04-14T19:05:44.524627Z","shell.execute_reply.started":"2024-04-14T19:05:44.517733Z","shell.execute_reply":"2024-04-14T19:05:44.523665Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"test_accuracy = cal_accuracy(y_test, y_test_pred)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T19:05:44.526097Z","iopub.execute_input":"2024-04-14T19:05:44.526463Z","iopub.status.idle":"2024-04-14T19:05:44.554566Z","shell.execute_reply.started":"2024-04-14T19:05:44.526426Z","shell.execute_reply":"2024-04-14T19:05:44.553725Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"markdown","source":"### 4) Test Results","metadata":{}},{"cell_type":"code","source":"print(\"test accuracy of model is : \", test_accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T19:05:44.555830Z","iopub.execute_input":"2024-04-14T19:05:44.556185Z","iopub.status.idle":"2024-04-14T19:05:44.561081Z","shell.execute_reply.started":"2024-04-14T19:05:44.556154Z","shell.execute_reply":"2024-04-14T19:05:44.560249Z"},"trusted":true},"execution_count":93,"outputs":[{"name":"stdout","text":"test accuracy of model is :  70.94619666048237\n","output_type":"stream"}]}]}